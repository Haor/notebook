# 神经网络
神经网络的一个重要性质是它可以自动地从数据中学习到合适的权重参数。

中间层有时候被称作隐藏层，因为它们的输出值并不直接给出结果，而是作为下一层的输入值。

## 激活函数
神经网络中的激活函数是非线性函数，它对信号做非线性变换。常用的激活函数有sigmoid函数、ReLU函数、Leaky ReLU函数、tanh函数等。  
激活函数的作用是引入非线性因素，使得神经网络可以表示更加复杂的函数。
它将输入信号的总和转换为输出信号。
输出值通常被称为激活值（activation）或信号值（signal）。
### 阶跃函数
阶跃函数是一种非线性函数，它的输出是0（当输入小于0时）或1（当输入大于0时）。
### sigmoid函数
sigmoid函数是一种S形曲线，它的输出是0到1之间的实数。
公式为：f(x) = 1 / (1 + exp(-x))
### ReLU函数
ReLU函数是指修正线性单元（Rectified Linear Unit）函数，它的输出是0到无穷大之间的实数。
它在输入大于0时，直接输出该值；在输入小于等于0时，输出0。
### Leaky ReLU函数
Leaky ReLU函数是指修正线性单元（Rectified Linear Unit）函数的一种变形，它的输出是0到无穷大之间的实数。  
它是在ReLU函数的基础上，对于输入小于0的情况，不再输出0，而是输出一个很小的值，比如0.01。
### tanh函数
tanh函数是指双曲正切函数，它的输出是-1到1之间的实数。  
公式为：f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))
### softmax函数
softmax函数，它的输出是0到1之间的实数，且和为1。  
softmax函数的特点是将输入信号的指数函数转换为0到1之间的实数。  
公式为：f(x) = exp(x) / sum(exp(x))
```
函数虽然看起来复杂，但是它其实只是给定某个输入后，会返回某个输出的转换器罢了。
```
```
激活函数是神经网络中的一种重要组成部分，它的作用是对网络中的输入信号进行非线性变换。常见的激活函数有:

Sigmoid函数： 使输入信号被压缩在0~1之间，适用于二分类问题
ReLU(Rectified Linear Unit)函数：常用于深度学习中的卷积层和全连接层，输出为max(0,x)
Tanh(hyperbolic tangent)函数： 使输入信号被压缩在-1~1之间
Softmax 函数：适用于多分类问题
Leaky ReLU 函数：在 ReLU 函数中，当输入信号为负数时，输出信号也为0，而Leaky ReLU函数可以让负数部分有一个很小的负数输出
还有一些其他的激活函数，如ELU，SELU等

不同的激活函数会对网络的性能有不同的影响，需要根据实际问题来选择合适的激活函数。
from chatgpt
```
实际上，感知机和神经网络的主要区别就在于激活函数。

### sigmoid函数和阶跃函数的比较
首先是“平滑性”的不同。sigmoid函数的输出是平滑的，而阶跃函数的输出是不连续的。  
sigmoid函数的平滑性使得它在梯度下降法中可以很好地工作，而阶跃函数的不连续性使得它在梯度下降法中无法很好地工作。  

其次是“输出值范围”的不同。  
sigmoid函数和阶跃函数都是非线性函数，但是它们的输出值的范围不同。sigmoid函数的输出是0到1之间的实数，而阶跃函数的输出是0或1。  

虽然在平滑性上有差异，但是从宏观来看，可以发现它们有相似的形状。  
当输入信号的值很小时，sigmoid函数和阶跃函数的输出都接近0；当输入信号的值很大时，sigmoid函数和阶跃函数的输出都接近1。  
也就是说，当输入信号为重要信息时，sigmoid函数和阶跃函数的输出都很大；当输入信号为不重要信息时，sigmoid函数和阶跃函数的输出都很小。  

### 非线性网络
神经网络的激活函数**必须**使用非线性函数，换句话说，激活函数不能使用线性函数。  
如果使用线性函数作为激活函数，那么神经网络就退化成了感知机，加深神经网络的层数就没有意义了。  
线性函数的问题在于，不管如何加深层数，总是存在与之等价的单层神经网络。

## 三层神经网络的实现
### 乘积运算的实现
乘积运算的实现可以使用numpy的dot函数，也可以使用for循环实现。  

输出层所用的激活函数，要根据问题的性质来选择。一般来说，回归问题使用恒等函数，二分类问题使用sigmoid函数，多分类问题使用softmax函数。
### 三层神经网络的实现
```
import numpy as np

def init_network():
    network = {}
    network['W1'] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])
    network['b1'] = np.array([0.1, 0.2, 0.3])
    network['W2'] = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])
    network['b2'] = np.array([0.1, 0.2])
    network['W3'] = np.array([[0.1, 0.3], [0.2, 0.4]])
    network['b3'] = np.array([0.1, 0.2])

    return network

def forward(network, x):
    W1, W2, W3 = network['W1'], network['W2'], network['W3']
    b1, b2, b3 = network['b1'], network['b2'], network['b3']

    a1 = np.dot(x, W1) + b1
    z1 = sigmoid(a1)
    a2 = np.dot(z1, W2) + b2
    z2 = sigmoid(a2)
    a3 = np.dot(z2, W3) + b3
    y = identity_function(a3)

    return y

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def identity_function(x):
    return x

network = init_network()
x = np.array([1.0, 0.5])
y = forward(network, x)
print(y)    # [ 0.31682708  0.69627909]
```

###恒等函数和softmax函数
恒等函数会将输入信号原封不动地输出，而softmax函数会将输入信号转换为概率。
```
def softmax(a):
    c = np.max(a)
    exp_a = np.exp(a - c)   # 溢出对策，为了防止指数函数的值过大，导致计算机无法处理。
    sum_exp_a = np.sum(exp_a)
    y = exp_a / sum_exp_a

    return y
```

softmax函数的输出是0.0到1.0的实数，且输出的总和为1。
**并且输出值的总和为1。**
这是softmax函数的重要特征，它表示了输出值的概率分布。正是因为这个性质，我们可以将softmax函数的输出解释为“**概率**”。
输出值的大小反映了该神经元的重要性。  
`求解机器学习问题的步骤可以分为“学习”和“推理”两个阶段。推理阶段一般会省略softmax函数，因为softmax函数的输出是概率，而推理阶段只需要识别出概率最高的那个元素。`

#### 输出层的神经元数量
输出层的神经元数量，一般与问题的类别数相同。
